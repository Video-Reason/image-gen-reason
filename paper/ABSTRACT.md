# Abstract

## Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven's Matrices

Recent advances in video generation have produced models capable of creating visually compelling videos, yet their capacity for visual reasoning—generating videos that demonstrate correct solutions to cognitive tasks—remains largely unexplored. We introduce VMEvalKit, a systematic evaluation framework that tests whether video generation models can reason through five fundamental cognitive challenges: chess puzzles (strategic planning), maze navigation (spatial pathfinding), Sudoku solving (logical deduction), 3D mental rotation (spatial transformation), and Raven's Progressive Matrices (abstract pattern recognition). Each task requires models to generate videos transitioning from an initial problem state to a correct solution state, moving beyond surface-level visual synthesis to demonstrate genuine reasoning capabilities.

**Main Contributions:**

**1. Emergent Reasoning in Video Models.** We provide the first systematic evidence that state-of-the-art video generation models exhibit measurable reasoning capabilities across diverse cognitive domains. By evaluating 40 models spanning 11 model families (including Sora, Veo, Luma, Runway, and open-source alternatives), we demonstrate that leading commercial models achieve success rates exceeding 60% on reasoning tasks, with top performers like Sora and Veo 3.1 solving complex chess tactics and abstract pattern completion at human-competitive levels. This represents a fundamental shift from video models as pure generative systems to systems capable of visual problem-solving.

**2. Validated Evaluation Paradigm.** We establish a robust experimental methodology centered on the "Task Pair" paradigm: (initial state image, final solution image, instruction prompt). This approach enables objective, automated evaluation by comparing generated video endpoints against ground truth solutions. Statistical validation demonstrates that our automated GPT-4O evaluation correlates strongly with human judgment (Pearson r=0.949, Cohen's κ=0.867, p=0.051), converging at as few as 25 samples. The paradigm's clarity and reproducibility make it suitable for standardized benchmarking of video reasoning capabilities at scale.

**3. Extensible Open-Source Framework.** We release VMEvalKit, a modular Python framework designed for systematic evaluation of reasoning in video generation models. The architecture supports dynamic model integration through a clean registry system (40 models across commercial APIs and open-source submodules), standardized task definition via the Task Pair interface, and automated evaluation with resume capability. The framework's design enables researchers to add new cognitive tasks (via a simple registry entry and dataset generator) and new models (via a wrapper class inheriting from ModelWrapper) without modifying core infrastructure, facilitating large-scale collaborative benchmarking.

**4. Robust Automated Evaluation.** We demonstrate that automated evaluation using vision-language models (GPT-4O) provides statistically equivalent assessments to human annotation while enabling evaluation at scale. Our validation across 450 paired human-GPT evaluations shows no significant difference in rating distributions (paired t-test p=0.051, Wilcoxon p=0.067) and substantial inter-rater agreement (Cohen's κ=0.867). Task-specific evaluation prompts achieve domain-specific validation accuracy, with particularly strong agreement on chess (r=0.985) and sudoku (r=0.997) tasks. This robustness enables the paradigm to scale to thousands of evaluations across many models and tasks without prohibitive human annotation costs.

**5. Foundation for Learning-Based Improvements.** The availability of objective, automated evaluation opens new research directions for improving video model reasoning through reinforcement learning. The Task Pair paradigm provides clear reward signals (solution correctness scores), ground truth targets for supervised fine-tuning, and a standardized benchmark for measuring improvements. We discuss how this evaluation infrastructure could enable iterative reasoning improvement through techniques like outcome-based reward modeling, video generation policy optimization, and multi-task reasoning transfer—transforming video models from passive generators into active problem solvers.

Our results reveal that modern video generation models possess latent reasoning capabilities that emerge when evaluated under appropriate paradigms. VMEvalKit provides both the empirical evidence of this emergence and the infrastructure to systematically measure, compare, and ultimately enhance reasoning in video models. By demonstrating that video generation is transitioning from photorealism to problem-solving, we establish a new frontier for video AI research where models must not only synthesize plausible worlds but also demonstrate understanding of the logical, spatial, and strategic principles that govern them.

---

**Code, dataset, and evaluation tools:** https://github.com/yourusername/VMEvalKit

