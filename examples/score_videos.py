import os
import sys
import logging
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent))

from vmevalkit.eval import HumanEvaluator, GPT4OEvaluator, InternVLEvaluator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def example_human_scoring():
    print("\n=== Human Scoring Example ===")
    print(f"Evaluating ENTIRE pilot experiment")
    print("Tasks with existing scorings will be automatically skipped")
    
    scorer = HumanEvaluator(
        experiment_name="pilot_experiment"
    )
    
    print(f"\nLaunching human scoring interface...")
    print("Enter your annotator name in the interface")
    scorer.launch_interface(port=7860, share=True)


def example_gpt4o_scoring():
    print("\n=== GPT-4O Scoring Example ===")
    print("ğŸ¤– Evaluating ENTIRE pilot experiment with GPT-4O")
    print("âš ï¸  Note: This will make API calls to OpenAI and may take time/cost money")
    print("âœ… Resume-capable: Interrupted scorings can be continued")
    
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("âŒ Error: Please set OPENAI_API_KEY environment variable")
    
    scorer = GPT4OEvaluator(
        experiment_name="pilot_experiment",
        temperature=0.0
    )
    
    eval_dir = Path("data/scorings/gpt4o-score/pilot_experiment")
    if eval_dir.exists():
        existing_files = list(eval_dir.rglob("*.json"))
        if existing_files:
            print(f"ğŸ“Š Found {len(existing_files)} existing GPT-4O scorings - will resume from where left off")
    
    print(f"\nğŸš€ Starting GPT-4O scoring on pilot_experiment...")
    print("ğŸ’¡ Tip: You can interrupt (Ctrl+C) and resume later - progress is saved after each task")
    
    try:
        all_results = scorer.evaluate_all_models()
        
        print("\nğŸ“ˆ GPT-4O EVALUATION RESULTS:")
        total_all = 0
        completed_all = 0
        for model_name, results in all_results.items():
            if "evaluations" in results:
                total_tasks = 0
                evaluated_tasks = 0
                for task_type, tasks in results["evaluations"].items():
                    for task_id, result in tasks.items():
                        total_tasks += 1
                        if "error" not in result and result.get("status") != "failed":
                            evaluated_tasks += 1
                
                total_all += total_tasks
                completed_all += evaluated_tasks
                
                status = "âœ… Complete" if evaluated_tasks == total_tasks else f"ğŸ”„ {evaluated_tasks}/{total_tasks}"
                print(f"  â€¢ {model_name}: {status}")
        
        print(f"\nğŸ‰ GPT-4O EVALUATION COMPLETE!")
        print(f"ğŸ“Š Total: {completed_all}/{total_all} tasks evaluated successfully")
        print(f"ğŸ’¾ Results saved to: data/scorings/gpt4o-score/pilot_experiment/")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  GPT-4O scoring interrupted!")
        print(f"ğŸ’¾ Progress has been saved. Run the same command again to resume.")
        print(f"ğŸ“ Partial results available in: data/scorings/gpt4o-score/pilot_experiment/")


def example_internvl_scoring():
    print("\n=== InternVL Scoring Example ===")
    print("ğŸ¤– Evaluating ENTIRE pilot experiment with InternVL")
    print("âš ï¸  Note: This will make API calls to local InternVL server")
    print("âœ… Resume-capable: Interrupted scorings can be continued")
    
    api_key = os.getenv("VISION_API_KEY", "YOUR_API_KEY")
    base_url = os.getenv("VISION_API_BASE", "http://0.0.0.0:23333/v1")
    
    if api_key == "YOUR_API_KEY":
        print("âš ï¸  Warning: Using default API key. Set VISION_API_KEY if needed.")
    
    scorer = InternVLEvaluator(
        experiment_name="pilot_experiment",
        api_key=api_key,
        base_url=base_url,
        temperature=0.0
    )
    
    eval_dir = Path("data/evaluations/vision-eval/pilot_experiment")
    if eval_dir.exists():
        existing_files = list(eval_dir.rglob("*.json"))
        if existing_files:
            print(f"ğŸ“Š Found {len(existing_files)} existing InternVL scorings - will resume from where left off")
    
    print(f"\nğŸš€ Starting InternVL scoring on pilot_experiment...")
    print(f"ğŸŒ Base URL: {base_url}")
    print("ğŸ’¡ Tip: You can interrupt (Ctrl+C) and resume later - progress is saved after each task")
    
    try:
        all_results = scorer.evaluate_all_models()
        
        print("\nğŸ“ˆ InternVL EVALUATION RESULTS:")
        total_all = 0
        completed_all = 0
        for model_name, results in all_results.items():
            if "evaluations" in results:
                total_tasks = 0
                evaluated_tasks = 0
                for task_type, tasks in results["evaluations"].items():
                    for task_id, result in tasks.items():
                        total_tasks += 1
                        if "error" not in result and result.get("status") != "failed":
                            evaluated_tasks += 1
                
                total_all += total_tasks
                completed_all += evaluated_tasks
                
                status = "âœ… Complete" if evaluated_tasks == total_tasks else f"ğŸ”„ {evaluated_tasks}/{total_tasks}"
                print(f"  â€¢ {model_name}: {status}")
        
        print(f"\nğŸ‰ InternVL EVALUATION COMPLETE!")
        print(f"ğŸ“Š Total: {completed_all}/{total_all} tasks evaluated successfully")
        print(f"ğŸ’¾ Results saved to: data/evaluations/vision-eval/pilot_experiment/")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  InternVL scoring interrupted!")
        print(f"ğŸ’¾ Progress has been saved. Run the same command again to resume.")
        print(f"ğŸ“ Partial results available in: data/evaluations/vision-eval/pilot_experiment/")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="VMEvalKit End-to-End Scoring Runner",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
        End-to-End Scoring Examples:
        # Run human scoring (automatically skips already evaluated tasks)
        python score_videos.py human
        
        Note: 
        - Tasks with existing scorings are automatically skipped
        - Annotator name is entered directly in the Gradio interface
        
        # Run GPT-4O scoring on ENTIRE pilot experiment
        python score_videos.py gpt4o
        
        # Run InternVL scoring on ENTIRE pilot experiment
        python score_videos.py internvl

        Note: All methods evaluate the complete pilot experiment (all models, all tasks).
        """
    )
    
    parser.add_argument(
        'method',
        choices=['human', 'gpt4o', 'internvl'],
        help='Scoring method to use'
    )
    
    
    args = parser.parse_args()
    

    if not Path("data/outputs/pilot_experiment").exists():
        print("Error: pilot_experiment not found. Please run inference first.")
        return
    
    if args.method == "human":
        example_human_scoring()
    elif args.method == "gpt4o":
        example_gpt4o_scoring()
    elif args.method == "internvl":
        example_internvl_scoring()


if __name__ == "__main__":
    main()